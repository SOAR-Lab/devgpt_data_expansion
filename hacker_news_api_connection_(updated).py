# -*- coding: utf-8 -*-
"""Hacker News API Connection (Updated).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V1SEfgCUCPFzM0bavfPDCHpprqegkljM

Endpoints:
* Top stories: /v0/topstories.json
* Story by ID: /v0/item/<item_id>.json

# Extracting chats using both regex patterns
"""

#import libraries
import requests
import re
import json
import numpy as np
import pandas as pd

# Defining base url
base_url = "http://hn.algolia.com/api/v1/search_by_date?query=chatgpt.com&restrictSearchableAttributes=url&created_at_i>1697155200&tags=story"

# List to store filtered results
filtered_chats = []

# Function to fetch comments by their IDs
def fetch_comments(comment_ids):
    comments = []
    for comment_id in comment_ids:
        comment_url = f"https://hacker-news.firebaseio.com/v0/item/{comment_id}.json?print=pretty"
        comment_response = requests.get(comment_url)

        if comment_response.status_code == 200:
            comment_data = comment_response.json()
            if "text" in comment_data:
                comment_text = comment_data["text"]
            else:
                comment_text = "No comment text available"
            comment_author = comment_data.get("by", "Unknown")
            comments.append({
                "author": comment_author,
                "text": comment_text
            })
        else:
            print(f"Failed to fetch comment with ID {comment_id}. Status Code: {comment_response.status_code}")
    return comments

# Function to check if the discussion page mentions OpenAI or ChatGPT
def check_discussion_for_keywords(discussion_url):
    keywords = ['chatgpt', 'openai']

    # Fetch the discussion page
    discussion_response = requests.get(discussion_url)

    if discussion_response.status_code == 200:
        # Parse the HTML content of the discussion page
        soup = BeautifulSoup(discussion_response.text, 'html.parser')

        # Search for mentions of keywords in the text of the page
        discussion_text = soup.get_text().lower()  # Convert to lower case for case-insensitive search

        # Check if any keyword is mentioned in the discussion
        for keyword in keywords:
            if keyword in discussion_text:
                print(f"Found mention of {keyword} in discussion: {discussion_url}")
                return True

    else:
        print(f"Failed to fetch discussion page: {discussion_url}")

    return False

# Fetch the first page to get the total number of pages (for pagination)
response = requests.get(base_url)
if response.status_code == 200:
    data = response.json()

    # Get the total number of pages
    total_pages = data["nbPages"]
    print(f"Total Pages: {total_pages}")

    # Regex pattern to match the URLs containing 'chatgpt.com/share/'
    pattern = r"https://chatgpt\.com/share/.*"

    # Loop through all pages
    for page in range(total_pages):
        url = f"{base_url}&page={page}"
        print(f"Fetching page {page + 1}...")

        response = requests.get(url)

        if response.status_code == 200:
            data = response.json()

            # Loop through the items on the current page
            for item in data["hits"]:
                # Get the URL of the item (if available)
                url = item.get("url", None)

                # Use regex to check if the URL matches the pattern
                chats_found = re.search(pattern, url)

                # If a URL is present and it matches the pattern
                if url and chats_found:
                    # Create a dictionary with relevant details from the item
                    filtered_item = {
                        "Author": item.get('author', 'Unknown'),
                        "Title": item.get('title', 'No title'),
                        "Url": url
                    }

                    # Y Combinator discussion page URL
                    story_id = item.get('objectID')
                    discussion_url = f"https://news.ycombinator.com/item?id={story_id}"
                    filtered_item["Discussion URL"] = discussion_url

                    # Check if the discussion contains 'chatgpt' or 'openai'
                    if check_discussion_for_keywords(discussion_url):
                        # Get the number of comments (length of the 'kids' field)
                        comment_ids = item.get("kids", [])
                        num_comments = len(comment_ids)
                        filtered_item["Num Comments"] = num_comments

                        # Initialize comments
                        comments = []

                        # Fetch the actual comments using the 'kids' field (if any)
                        if comment_ids:
                            comments = fetch_comments(comment_ids)
                            filtered_item["Comments"] = comments
                        else:
                            print(f"No comments found for story: {item.get('title', 'No title')}")

                        # Add the filtered item to the list
                        filtered_chats.append(filtered_item)

                        # Printing the filtered information
                        print(f"Author: {item.get('author', 'Unknown')}")
                        print(f"Title: {item.get('title', 'No title')}")
                        print(f"Url: {url}")
                        print(f"Discussion URL: {discussion_url}")
                        print(f"Number of Comments: {num_comments}")
                        for comment in comments:
                            print(f"Comment by {comment['author']}: {comment['text']}")

        else:
            print(f"Failed to fetch page {page + 1}. Status Code:", response.status_code)

    # Total number of filtered results
    print(f"Total filtered items: {len(filtered_chats)}")

#saving to a json file
with open('filtered_data.json', 'w') as json_file:
  json.dump(filtered_chats, json_file, indent=4)

"""# openai"""

# Defining base url
base_url = "http://hn.algolia.com/api/v1/search_by_date?query=chat.openai.com&restrictSearchableAttributes=url&tags=story&created_at_i>1697155200"

# List to store filtered results
filtered_chats = []

# Function to fetch comments by their IDs
def fetch_comments(comment_ids):
    comments = []
    for comment_id in comment_ids:
        comment_url = f"https://hacker-news.firebaseio.com/v0/item/{comment_id}.json?print=pretty"
        comment_response = requests.get(comment_url)

        if comment_response.status_code == 200:
            comment_data = comment_response.json()
            if "text" in comment_data:
                comment_text = comment_data["text"]
            else:
                comment_text = "No comment text available"
            comment_author = comment_data.get("by", "Unknown")
            comments.append({
                "author": comment_author,
                "text": comment_text
            })
        else:
            print(f"Failed to fetch comment with ID {comment_id}. Status Code: {comment_response.status_code}")
    return comments

# Function to check if the discussion page mentions OpenAI or ChatGPT
def check_discussion_for_keywords(discussion_url):
    keywords = ['chatgpt', 'openai']

    # Fetch the discussion page
    discussion_response = requests.get(discussion_url)

    if discussion_response.status_code == 200:
        # Parse the HTML content of the discussion page
        soup = BeautifulSoup(discussion_response.text, 'html.parser')

        # Search for mentions of keywords in the text of the page
        discussion_text = soup.get_text().lower()  # Convert to lower case for case-insensitive search

        # Check if any keyword is mentioned in the discussion
        for keyword in keywords:
            if keyword in discussion_text:
                print(f"Found mention of {keyword} in discussion: {discussion_url}")
                return True

    else:
        print(f"Failed to fetch discussion page: {discussion_url}")

    return False

# Fetch the first page to get the total number of pages (for pagination)
response = requests.get(base_url)
if response.status_code == 200:
    data = response.json()

    # Get the total number of pages
    total_pages = data["nbPages"]
    print(f"Total Pages: {total_pages}")

    # Regex pattern to match the URLs containing 'chatgpt.com/share/'
    pattern = r"https://chat\.openai\.com/share/.*"

    # Loop through all pages
    for page in range(total_pages):
        url = f"{base_url}&page={page}"
        print(f"Fetching page {page + 1}...")

        response = requests.get(url)

        if response.status_code == 200:
            data = response.json()

            # Loop through the items on the current page
            for item in data["hits"]:
                # Get the URL of the item (if available)
                url = item.get("url", None)

                # Use regex to check if the URL matches the pattern
                chats_found = re.search(pattern, url)

                # If a URL is present and it matches the pattern
                if url and chats_found:
                    # Create a dictionary with relevant details from the item
                    filtered_item = {
                        "Author": item.get('author', 'Unknown'),
                        "Title": item.get('title', 'No title'),
                        "Url": url
                    }

                    # Y Combinator discussion page URL
                    story_id = item.get('objectID')
                    discussion_url = f"https://news.ycombinator.com/item?id={story_id}"
                    filtered_item["Discussion URL"] = discussion_url

                    # Check if the discussion contains 'chatgpt' or 'openai'
                    if check_discussion_for_keywords(discussion_url):
                        # Get the number of comments (length of the 'kids' field)
                        comment_ids = item.get("kids", [])
                        num_comments = len(comment_ids)
                        filtered_item["Num Comments"] = num_comments

                        # Initialize comments
                        comments = []

                        # Fetch the actual comments using the 'kids' field (if any)
                        if comment_ids:
                            comments = fetch_comments(comment_ids)
                            filtered_item["Comments"] = comments
                        else:
                            print(f"No comments found for story: {item.get('title', 'No title')}")

                        # Add the filtered item to the list
                        filtered_chats.append(filtered_item)

                        # Printing the filtered information
                        print(f"Author: {item.get('author', 'Unknown')}")
                        print(f"Title: {item.get('title', 'No title')}")
                        print(f"Url: {url}")
                        print(f"Discussion URL: {discussion_url}")
                        print(f"Number of Comments: {num_comments}")
                        for comment in comments:
                            print(f"Comment by {comment['author']}: {comment['text']}")

        else:
            print(f"Failed to fetch page {page + 1}. Status Code:", response.status_code)

    # Total number of filtered results
    print(f"Total filtered items: {len(filtered_chats)}")

#saving to a json file
with open('filtered_data.json', 'w') as json_file:
    json.dump(filtered_chats, json_file, indent=4)

#Get all data consisting of chatgpt
response = requests.get("http://hn.algolia.com/api/v1/search_by_date",
                        params = {"query":"chatgpt",
                                  "tags":"story",
                                  #"restrictSearchableAttributes":"story_text",
                                  "hitsPerPage" : "1000",
                                  "numericFilters":"created_at_i>1697155200"}) # unix timestamp for 10/13/2023

response.json()

filtered_chats = []
data = response.json()
for item in data["hits"]:
  if 'chatgpt' in item.get('title','').lower() or 'chatgpt' in item.get('text','').lower():
    url = item.get('url','No URL')
    filtered_item = {
                      "Author" : item['author'],
                      "Title" : item['title'],
                      "Url" : url
                      }
    filtered_chats.append(filtered_item)

    # Printing the filtered information
    print(f"Author: {item['author']}")
    print(f"Title: {item['title']}")
    print(f"Url: {url}")

# Calculate and print the total number of URLs
total_urls = len(filtered_chats)
print(f"Total URLs pulled: {total_urls}")

#saving to a json file
with open('filtered_data.json', 'w') as json_file:
    json.dump(filtered_chats, json_file, indent=4)